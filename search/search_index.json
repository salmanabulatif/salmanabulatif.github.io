{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"2025/07/13/neural-network/","title":"Neural Network","text":"<p>Neural networks rely on two key mathematical processes: forward propagation (for predictions) and backward propagation (for learning via gradient descent).</p>"},{"location":"2025/07/13/neural-network/#the-math-behind-neural-network-forward-and-backward-propagation","title":"The Math Behind Neural Network Forward and Backward Propagation","text":"<p>Neural networks rely on two key mathematical processes: forward propagation (for predictions) and backward propagation (for learning via gradient descent). In this post, we break down the core math behind these steps using a simple 3-layer neural network example. Hand-written calculations and diagrams from my notes are included for clarity.</p>"},{"location":"2025/07/13/neural-network/#network-architecture","title":"Network Architecture","text":"<p>We consider a neural network with:</p> <ul> <li>Input layer: 3 neurons (<code>x\u2081</code>, <code>x\u2082</code>, <code>x\u2083</code>)</li> <li>Hidden layer: 2 neurons (<code>h\u2084</code>, <code>h\u2085</code>)</li> <li>Output layer: 1 neuron (<code>o\u2086</code>)</li> </ul> <pre><code>Input Layer (x\u2081, x\u2082, x\u2083) \u2192 Hidden Layer (h\u2084, h\u2085) \u2192 Output Layer (o\u2086)\n</code></pre> <p></p>"},{"location":"2025/07/13/neural-network/#1-forward-pass","title":"1. Forward Pass","text":"<p>The forward pass computes the output of each neuron layer-by-layer using weights (<code>\u03c9</code>), biases (<code>b</code>), and activation functions (sigmoid here).</p>"},{"location":"2025/07/13/neural-network/#hidden-layer-calculations","title":"Hidden Layer Calculations","text":"<p>For neuron 4 (<code>h\u2084</code>):</p>  a_4 = \\sigma(\\text{inputs} \\cdot \\text{weights} + b_4)  <p>From the notes:</p>  a_4 = \\frac{1}{1 + e^{-(-0.7)}} = 0.332  <p>For neuron 5 (<code>h\u2085</code>):</p>  a_5 = \\frac{1}{1 + e^{-0.1}} = 0.325"},{"location":"2025/07/13/neural-network/#output-layer-calculation","title":"Output Layer Calculation","text":"o_6 = \\sigma(a_4 \\omega_{46} + a_5 \\omega_{56} + b_6)   a_6 = \\frac{1}{1 + e^{-(-0.206)}} = 0.194"},{"location":"2025/07/13/neural-network/#2-loss-calculation","title":"2. Loss Calculation","text":"<p>The error (loss) is computed between the prediction (<code>o\u2086</code>) and the target value.</p> <p>From the notes (target = 1):</p>  \\text{Error} = 1 - 0.414 = 0.526"},{"location":"2025/07/13/neural-network/#3-backward-propagation","title":"3. Backward Propagation","text":"<p>Backward propagation applies the chain rule to compute gradients for each weight and bias.</p>"},{"location":"2025/07/13/neural-network/#output-neuron-gradient-delta_6delta_6","title":"Output Neuron Gradient (\\delta_6)","text":"\\delta_6 = \\text{Error} \\times \\sigma'(o_6)  <p>where </p>  \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))  <p>From the notes:</p>  \\delta_6 = 0.414 \\times (1 - 0.414) \\times (1 - 0.414) = 0.131"},{"location":"2025/07/13/neural-network/#hidden-neuron-gradients","title":"Hidden Neuron Gradients","text":"<p>For neuron 5 (\\delta_5):</p>  \\delta_5 = a_5(1 - a_5) \\times (\\delta_6 \\cdot \\omega_{56})   \\delta_5 = 0.325 \\times (1 - 0.525) \\times (0.2 \\times 0.131) = 0.095  <p>For neuron 4 (\\delta_4):</p>  \\delta_4 = a_4(1 - a_4) \\times (\\delta_6 \\cdot \\omega_{46})   \\delta_4 = 0.332 \\times (1 - 0.332) \\times (0.2 \\times 0.131) = 0.095  <p></p>"},{"location":"2025/07/13/neural-network/#4-weight-updates","title":"4. Weight Updates","text":"<p>Weights are updated using the computed gradients and a learning rate (\\eta=0.9).</p> <p>Update Rule:</p>  \\Delta \\omega_{ij} = \\eta \\times \\delta_i \\times a_j   \\omega_{ij}^{\\text{new}} = \\omega_{ij}^{\\text{old}} + \\Delta \\omega_{ij}  <p>Example Updates (see notes):</p> <ul> <li> <p>Weight \\omega_{16} (input 1 \u2192 output 6):</p>  \\Delta \\omega_{16} = 0.9 \\times 0.131 \\times 0.392 = 0.046   \\omega_{16}^{\\text{new}} = 0.939 - 0.3 = 0.639  </li> <li> <p>Weight \\omega_{35} (input 3 \u2192 hidden 5):</p>  \\Delta \\omega_{35} = 0.9 \\times 0.00653 \\times 1 = 0.00588   \\omega_{35}^{\\text{new}} = 0.2 - 0.00857 = 0.19143  </li> </ul>"},{"location":"2025/07/13/neural-network/#5-bias-updates","title":"5. Bias Updates","text":"<p>Biases are updated similarly:</p>  b_i^{\\text{new}} = b_i^{\\text{old}} + \\eta \\times \\delta_i  <p>For output neuron 6:</p>  b_6^{\\text{new}} = 0.1 + (0.9 \\times 0.131) = 0.218  <p></p>"},{"location":"2025/07/13/neural-network/#key-equations","title":"Key Equations","text":"Component Equation Forward Pass  a_j = \\sigma\\left(\\sum_i \\omega_{ij} x_i + b_j\\right)  Sigmoid  \\sigma(z) = \\dfrac{1}{1 + e^{-z}}  Output Gradient  \\delta_k = (y - \\hat{y}) \\cdot \\sigma'(o_k)  Hidden Gradient  \\delta_j = \\sigma'(a_j) \\sum_k (\\delta_k \\omega_{jk})  Weight Update  \\Delta \\omega_{ij} = \\eta \\cdot \\delta_j \\cdot a_i  Bias Update  \\Delta b_j = \\eta \\cdot \\delta_j"},{"location":"2025/07/13/neural-network/#why-this-matters","title":"Why This Matters","text":"<p>Backpropagation efficiently computes gradients by:</p> <ol> <li>Forward pass: Calculate predictions and cache values.</li> <li>Backward pass: Apply the chain rule for error gradients.</li> <li>Update: Adjust weights and biases to minimize loss.</li> </ol> <p>Your files and these notes demonstrate a practical implementation of these principles\u2014calculating gradients and updating weights. While the math may appear complex, it systematically optimizes the network using calculus and linear algebra.</p> <p>Handwritten notes and calculations for visual reference:</p> <ul> <li>Network diagram: </li> <li>Forward pass &amp; error calculation: </li> <li>Backpropagation and weight update math: </li> <li>Bias update math: </li> </ul> <p>References: - Neural Networks and Deep Learning - CS231n: Convolutional Neural Networks for Visual Recognition (Stanford)</p>"},{"location":"2025/07/12/snr-blog/","title":"SNR Blog","text":"<p>Welcome to the blog for the Wi-Fi Proximity Sensor project, where we transform a standard Wi-Fi adapter into a real-time proximity detector using machine learning.</p>"},{"location":"2025/07/12/snr-blog/#wi-fi-proximity-sensor-with-machine-learning-a-hackers-guide","title":"Wi-Fi Proximity Sensor with Machine Learning: A Hacker's Guide","text":"<p>Hey hackers and pentesters! Want to turn a cheap Wi-Fi adapter into a stealthy proximity sensor for physical security assessments or red team ops? This Python-based project leverages wireless signal metrics\u2014signal strength, noise level, and Signal-to-Noise Ratio (SNR)\u2014to estimate distances to nearby objects or people. Whether you're detecting movement in a target environment, building covert presence detection, or integrating with IoT for sneaky automation, this tool has you covered. It combines raw signal processing with a machine learning pipeline for precision and adaptability.</p> <p>The full source code will drop soon on GitHub (link coming soon).</p>"},{"location":"2025/07/12/snr-blog/#project-overview","title":"Project Overview","text":"<p>This Wi-Fi Proximity Sensor runs on Linux, sniffing metrics from a Wi-Fi interface to estimate proximity. It supports two modes: static threshold-based detection for quick setups, and a machine learning mode that trains on custom data for environment-specific accuracy. Built with Python 3, <code>scikit-learn</code>, and <code>joblib</code>, it uses system tools (<code>iw</code>, <code>/proc/net/wireless</code>, <code>iwconfig</code>) to pull raw signal data.</p> <p>For pentesters, this is a lightweight, hackable tool for: - Physical security testing - Rogue device detection - Social engineering ops where proximity matters</p>"},{"location":"2025/07/12/snr-blog/#key-features","title":"Key Features","text":"<ul> <li>Metric Sniffing: Pulls signal strength and noise using multiple Linux tools for reliability across setups.</li> <li>Proximity Estimation: Maps SNR to proximity zones (\"Very Close,\" \"Normal Range,\" \"Moving Away,\" \"Far Away\") or exact distances via ML.</li> <li>Signal Smoothing: Uses exponential moving average (EMA) to stabilize noisy Wi-Fi signals.</li> <li>Machine Learning: Collects data and trains regression (<code>LinearRegression</code>) or classification (<code>SVC</code>) models for custom environments.</li> <li>Real-Time Monitoring: Low-latency updates for dynamic scenarios like tailgating detection.</li> <li>Pentesting Applications: Think intrusion detection, physical access monitoring, or IoT exploitation.</li> </ul>"},{"location":"2025/07/12/snr-blog/#technical-breakdown","title":"Technical Breakdown","text":"<p>The system is built for hackers who love to tinker. It operates in four modes:</p> <ol> <li>Monitoring Mode: Real-time tracking of Wi-Fi metrics with proximity estimates using static thresholds or a trained model.</li> <li>Data Collection Mode: Logs signal data with ground-truth distances or labels for training custom models.</li> <li>Model Training Mode: Builds a <code>scikit-learn</code> pipeline to predict distances or proximity zones.</li> <li>Inference Mode: Deploys the trained model for live, environment-tuned proximity detection.</li> </ol>"},{"location":"2025/07/12/snr-blog/#signal-processing","title":"Signal Processing","text":"<p>The system scrapes Wi-Fi metrics using a tiered approach to handle diverse hardware:</p> <ul> <li><code>iw dev &lt;interface&gt; link</code>: Modern, clean way to get signal strength (dBm).</li> <li><code>/proc/net/wireless</code>: Kernel-level stats for signal and noise, great for older systems.</li> <li><code>iwconfig</code>: Fallback for legacy setups, parsing signal/noise from text output.</li> </ul> <p>SNR Calculation:</p> <pre><code>SNR = Signal Strength (dBm) - Noise Level (dBm)\n</code></pre> <p>If noise is unavailable (common in some adapters), it approximates:</p> <pre><code>SNR \u2248 Signal Strength + 90\n</code></pre> <p>Signal Smoothing (EMA, \u03b1 = 0.7):</p> <pre><code>SNR_smoothed_t = \u03b1 \u00b7 SNR_smoothed_{t-1} + (1-\u03b1) \u00b7 SNR_raw_t\n</code></pre> <p>This keeps readings stable for reliable proximity estimates during pentests.</p>"},{"location":"2025/07/12/snr-blog/#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<p>For hackers, the ML component is where things get juicy. Feature vector includes:</p> <ul> <li>Signal strength (dBm, default -100 if missing)</li> <li>Noise level (dBm, default -90 if missing)</li> <li>Smoothed SNR (dB, default 0 if missing)</li> </ul> <p>The system auto-detects label type:</p> <ul> <li>Regression (<code>LinearRegression</code>): Predicts distance (cm), mapped to zones:</li> <li>&lt;50 cm: \"VERY CLOSE\"</li> <li>50\u2013200 cm: \"NORMAL RANGE\"</li> <li>200\u2013400 cm: \"MOVING AWAY\"</li> <li> <p>400 cm: \"FAR AWAY\"</p> </li> <li>Classification (<code>SVC</code> with probability): Directly predicts proximity zones.</li> </ul> <p>A <code>StandardScaler</code> normalizes features to handle varying signal ranges. Models are serialized with <code>joblib</code> for quick deployment.</p> <p>Pentesters can train models in specific environments (e.g., target office) to account for walls, interference, or hardware quirks.</p>"},{"location":"2025/07/12/snr-blog/#pentesting-applications","title":"Pentesting Applications","text":"<ul> <li>Physical Intrusion Detection: Detect tailgating or unauthorized access by monitoring SNR changes near doors or secure areas.</li> <li>Rogue Device Tracking: Identify movement of Wi-Fi-enabled devices (e.g., laptops, phones) in restricted zones.</li> <li>IoT Exploitation: Integrate with MQTT or Home Assistant to trigger payloads when someone approaches.</li> <li>Social Engineering: Use proximity data to time physical access attempts or device interactions.</li> </ul>"},{"location":"2025/07/12/snr-blog/#core-code-preview","title":"Core Code (Preview)","text":"<p>Here\u2019s a stripped-down version of the core logic, focusing on metric extraction and monitoring. The full code, with data collection and training, will be released soon.</p> <pre><code>#!/usr/bin/env python3\nimport subprocess\nimport re\nimport time\nimport joblib\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nINTERFACE = \"wlan0\"\nSAMPLING_RATE = 0.5\nSMOOTHING_FACTOR = 0.7\nMODEL_FILE = \"wifi_proximity_model.pkl\"\n\ndef get_wireless_metrics(interface):\n    metrics = {'signal': None, 'noise': None}\n    try:\n        output = subprocess.check_output([\"iw\", \"dev\", interface, \"link\"], text=True, stderr=subprocess.DEVNULL)\n        sig_match = re.search(r\"signal:\\s*(-?\\d+)\\s*dBm\", output)\n        if sig_match:\n            metrics['signal'] = int(sig_match.group(1))\n    except Exception:\n        try:\n            with open(\"/proc/net/wireless\", \"r\") as f:\n                for line in f.readlines()[2:]:\n                    if interface in line:\n                        parts = line.strip().split()\n                        if len(parts) &gt;= 5:\n                            metrics['signal'] = int(float(parts[3]))\n                            metrics['noise'] = int(float(parts[4]))\n                            break\n        except Exception:\n            pass\n    return metrics\n\ndef calculate_snr(metrics):\n    if metrics['signal'] is not None and metrics['noise'] is not None:\n        return metrics['signal'] - metrics['noise']\n    elif metrics['signal'] is not None:\n        return metrics['signal'] + 90\n    return None\n\ndef monitor(interface, model=None):\n    smoothed_snr = None\n    print(f\"{'Time':&lt;8} | {'Signal':&gt;7} | {'SNR':&gt;6} | {'Status':&lt;25}\")\n    print(\"-\" * 50)\n    try:\n        while True:\n            timestamp = time.strftime(\"%H:%M:%S\")\n            metrics = get_wireless_metrics(interface)\n            snr = calculate_snr(metrics)\n            if snr is not None:\n                smoothed_snr = snr if smoothed_snr is None else SMOOTHING_FACTOR * smoothed_snr + (1 - SMOOTHING_FACTOR) * snr\n            signal_str = f\"{metrics['signal']} dBm\" if metrics['signal'] is not None else \"N/A\"\n            snr_str = f\"{smoothed_snr:.1f} dB\" if smoothed_snr is not None else \"N/A\"\n            status = \"NO SIGNAL\"\n            if snr is not None:\n                if model:\n                    features = [[metrics['signal'] or -100, metrics['noise'] or -90, smoothed_snr or 0]]\n                    pred = model.predict(features)[0]\n                    status = f\"{pred:.1f} cm\" if isinstance(pred, float) else pred\n                else:\n                    status = (\"VERY CLOSE\" if smoothed_snr &lt; 26 else\n                              \"NORMAL RANGE\" if smoothed_snr &lt; 33 else\n                              \"MOVING AWAY\" if smoothed_snr &lt; 40 else\n                              \"FAR AWAY\")\n            print(f\"{timestamp:&lt;8} | {signal_str:&gt;7} | {snr_str:&gt;6} | {status:&lt;25}\", end=\"\\r\")\n            time.sleep(SAMPLING_RATE)\n    except KeyboardInterrupt:\n        print(\"\\nMonitoring stopped.\")\n</code></pre> <p>This code is robust, with fallbacks for metric extraction and support for both static and ML-based detection. Hackers can extend it to log data to a remote server or trigger scripts on proximity events.</p>"},{"location":"2025/07/12/snr-blog/#setup-and-usage","title":"Setup and Usage","text":""},{"location":"2025/07/12/snr-blog/#requirements","title":"Requirements","text":"<ul> <li>OS: Linux (Kali, Ubuntu, etc.) with a Wi-Fi adapter (monitor mode not required but useful).</li> <li>Software: Python 3.6+, <code>scikit-learn</code>, <code>joblib</code>, <code>numpy</code>.</li> <li>Tools: <code>iw</code>, <code>iwconfig</code>, and read access to <code>/proc/net/wireless</code>.</li> </ul> <p>Install dependencies:</p> <pre><code>pip install scikit-learn joblib numpy\n</code></pre> <p>Check your Wi-Fi interface:</p> <pre><code>ip link show\n</code></pre>"},{"location":"2025/07/12/snr-blog/#commands","title":"Commands","text":"<ul> <li> <p>Monitor Mode: <pre><code>python3 wifi_proximity.py --interface wlan0\n</code></pre>   Uses <code>wifi_proximity_model.pkl</code> if present; falls back to static thresholds.</p> </li> <li> <p>Data Collection Mode: <pre><code>python3 wifi_proximity.py --collect --interface wlan0\n</code></pre>   Input distances (cm) or labels (e.g., \"VERY CLOSE\"). Saves to <code>wifi_prox_data.csv</code>.</p> </li> <li> <p>Model Training Mode: <pre><code>python3 wifi_proximity.py --train\n</code></pre>   Trains and saves the model to <code>wifi_proximity_model.pkl</code>.</p> </li> </ul>"},{"location":"2025/07/12/snr-blog/#sample-output","title":"Sample Output","text":"<p>Monitoring output looks like:</p> <pre><code>Time     | Signal  | SNR    | Status\n------------------------------------------------\n12:34:56 | -50 dBm | 40.0 dB | FAR AWAY\n12:34:57 | -48 dBm | 42.0 dB | FAR AWAY\n</code></pre> <p>With an ML model, you might see distances (e.g., \"150.2 cm\") for regression.</p>"},{"location":"2025/07/12/snr-blog/#hacking-tips-and-tricks","title":"Hacking Tips and Tricks","text":"<ul> <li>Spoofing Defense: Wi-Fi signals can be manipulated (e.g., via signal amplifiers). Train models in the target environment to detect anomalies in SNR patterns.</li> <li>Stealth Mode: Run in a Docker container or Raspberry Pi for covert deployment. Redirect output to a log file or C2 server with <code>tee</code> or <code>nc</code>.</li> <li>Model Evasion: Test model robustness by introducing noise (e.g., using <code>aircrack-ng</code> to simulate interference). Retrain with adversarial data to harden the model.</li> <li>Integration: Pipe proximity events to <code>metasploit</code> or <code>nmap</code> scripts for automated pentesting workflows.</li> <li>Hardware Mods: Use high-gain antennas to extend range or directional antennas for precise tracking.</li> </ul>"},{"location":"2025/07/12/snr-blog/#challenges-and-limitations","title":"Challenges and Limitations","text":"<ul> <li>Signal Noise: Multipath effects, walls, and interference can skew SNR. Train models with diverse data to compensate.</li> <li>Hardware Variability: Some adapters don\u2019t report noise, forcing reliance on the fallback SNR formula. Test on target hardware.</li> <li>ML Dependence: Model accuracy hinges on training data quality. Collect data in realistic conditions (e.g., during a recon phase).</li> <li>Latency: The 0.5s sampling rate may miss rapid movements. Tune <code>SAMPLING_RATE</code> for faster response at the cost of stability.</li> </ul>"},{"location":"2025/07/12/snr-blog/#future-enhancements-for-pentesters","title":"Future Enhancements for Pentesters","text":"<ul> <li>Multi-Adapter Support: Combine multiple Wi-Fi interfaces for triangulation or redundancy.</li> <li>Packet Injection: Integrate with <code>scapy</code> to correlate proximity with specific devices (MAC addresses).</li> <li>C2 Integration: Stream data to a command-and-control server for remote monitoring.</li> <li>ML Upgrades: Swap <code>LinearRegression</code>/<code>SVC</code> for XGBoost or neural nets for better accuracy.</li> <li>Web Dashboard: Build a Flask-based UI for real-time visualization during engagements.</li> </ul>"},{"location":"2025/07/12/snr-blog/#conclusion","title":"Conclusion","text":"<p>This Wi-Fi Proximity Sensor is a hacker\u2019s dream for physical security testing. It\u2019s lightweight, customizable, and perfect for detecting movement, triggering payloads, or enhancing red team ops. Whether you\u2019re sneaking through a facility or building a stealthy IoT trap, this tool delivers.</p> <p>The full code will hit GitHub (link coming soon), so stay tuned. Grab a Wi-Fi adapter, fire up Kali, and start hacking proximity like a pro!</p>"},{"location":"archive/2025/","title":"2025","text":""}]}